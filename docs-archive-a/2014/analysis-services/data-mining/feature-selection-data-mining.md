---
title: 機能の選択 (データマイニング) |Microsoft Docs
ms.custom: ''
ms.date: 03/06/2017
ms.prod: sql-server-2014
ms.reviewer: ''
ms.technology: analysis-services
ms.topic: conceptual
helpviewer_keywords:
- mining models [Analysis Services], feature selections
- attributes [data mining]
- feature selection algorithms [Analysis Services]
- data mining [Analysis Services], feature selections
- neural network algorithms [Analysis Services]
- naive bayes algorithms [Analysis Services]
- decision tree algorithms [Analysis Services]
- datasets [Analysis Services]
- clustering algorithms [Analysis Services]
- coding [Data Mining]
ms.assetid: b044e785-4875-45ab-8ae4-cd3b4e3033bb
author: minewiskan
ms.author: owend
ms.openlocfilehash: ef5ee56636e7710074a893aed733905e1bf983bd
ms.sourcegitcommit: ad4d92dce894592a259721a1571b1d8736abacdb
ms.translationtype: MT
ms.contentlocale: ja-JP
ms.lasthandoff: 08/04/2020
ms.locfileid: "87642416"
---
# <a name="feature-selection-data-mining"></a><span data-ttu-id="88717-102">機能の選択 (データ マイニング)</span><span class="sxs-lookup"><span data-stu-id="88717-102">Feature Selection (Data Mining)</span></span>
  <span data-ttu-id="88717-103">*機能の選択*は、データマイニングで一般的に使用される用語であり、処理と分析のために管理しやすいサイズに入力を減らすために使用できるツールと手法を記述します。</span><span class="sxs-lookup"><span data-stu-id="88717-103">*Feature selection* is a term commonly used in data mining to describe the tools and techniques available for reducing inputs to a manageable size for processing and analysis.</span></span> <span data-ttu-id="88717-104">特徴選択は、*カーディナリティの削減*だけではなく、モデルの構築時に考慮できる属性の数に対して任意または事前に定義されたカットオフを強制することを意味します。つまり、アナリストまたはモデリングツールは、分析のためにその有用性に基づいて属性をアクティブに選択または破棄します。</span><span class="sxs-lookup"><span data-stu-id="88717-104">Feature selection implies not only *cardinality reduction*, which means imposing an arbitrary or predefined cutoff on the number of attributes that can be considered when building a model, but also the choice of attributes, meaning that either the analyst or the modeling tool actively selects or discards attributes based on their usefulness for analysis.</span></span>  
  
 <span data-ttu-id="88717-105">機能の選択を適用できることは、効果的な分析にとって重要です。それは、モデルの作成に必要な情報以上の情報がデータセットに含まれていることがよくあるためです。</span><span class="sxs-lookup"><span data-stu-id="88717-105">The ability to apply feature selection is critical for effective analysis, because datasets frequently contain far more information than is needed to build the model.</span></span> <span data-ttu-id="88717-106">たとえば、顧客の特性を説明する 500 列が含まれているデータセットで、一部の列のデータがわずかしかなく、それらをモデルに追加しても得られる利点がかなり少ない場合があります。</span><span class="sxs-lookup"><span data-stu-id="88717-106">For example, a dataset might contain 500 columns that describe the characteristics of customers, but if the data in some of the columns is very sparse you would gain very little benefit from adding them to the model.</span></span> <span data-ttu-id="88717-107">不要な列を保持したままモデルを作成すると、トレーニング処理時により多くの CPU リソースとメモリが必要となり、完成したモデルに必要な記憶領域も増大します。</span><span class="sxs-lookup"><span data-stu-id="88717-107">If you keep the unneeded columns while building the model, more CPU and memory are required during the training process, and more storage space is required for the completed model.</span></span>  
  
 <span data-ttu-id="88717-108">リソースが問題とならない場合でも、不要な列を削除することをお勧めします。次の理由から、検出されるパターンの品質が低下する可能性があるためです。</span><span class="sxs-lookup"><span data-stu-id="88717-108">Even if resources are not an issue, you typically want to remove unneeded columns because they might degrade the quality of discovered patterns, for the following reasons:</span></span>  
  
-   <span data-ttu-id="88717-109">ノイズになる列や冗長な列が含まれる場合があります。</span><span class="sxs-lookup"><span data-stu-id="88717-109">Some columns are noisy or redundant.</span></span> <span data-ttu-id="88717-110">こうした列が含まれていると、データから意味のあるパターンを見つけるのが困難になります。</span><span class="sxs-lookup"><span data-stu-id="88717-110">This noise makes it more difficult to discover meaningful patterns from the data;</span></span>  
  
-   <span data-ttu-id="88717-111">ほとんどのデータ マイニング アルゴリズムでは、高次元のデータセットで高品質なパターンを検出するにははるかに大きなトレーニング データセットが必要になります。</span><span class="sxs-lookup"><span data-stu-id="88717-111">To discover quality patterns, most data mining algorithms require much larger training data set on high-dimensional data set.</span></span> <span data-ttu-id="88717-112">しかし、ごく小さなトレーニング データしかないデータ マイニング アプリケーションもあります。</span><span class="sxs-lookup"><span data-stu-id="88717-112">But the training data is very small in some data mining applications.</span></span>  
  
 <span data-ttu-id="88717-113">データ ソースの 500 列中、モデルの構築に有用な情報が含まれている列が 50 列のみである場合、それらを単純にモデルから削除することもできますが、機能の選択の技法を使用して最適な機能を自動的に見つけ、統計的に重要でない値を除外することもできます。</span><span class="sxs-lookup"><span data-stu-id="88717-113">If only 50 of the 500 columns in the data source have information that is useful in building a model, you could just leave them out of the model, or you could use feature selection techniques to automatically discover the best features and to exclude values that are statistically insignificant.</span></span> <span data-ttu-id="88717-114">機能の選択は、価値の低いデータが多すぎること、または価値の高いデータが少なすぎることという、2 つの問題を解決するために役立ちます。</span><span class="sxs-lookup"><span data-stu-id="88717-114">Feature selection helps solve the twin problems of having too much data that is of little value, or having too little data that is of high value.</span></span>  
  
## <a name="feature-selection-in-analysis-services-data-mining"></a><span data-ttu-id="88717-115">Analysis Services によるデータ マイニングでの機能の選択</span><span class="sxs-lookup"><span data-stu-id="88717-115">Feature Selection in Analysis Services Data Mining</span></span>  
 <span data-ttu-id="88717-116">通常、機能の選択は [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] で自動的に実行され、各アルゴリズムには、機能の削減を適切に適用する一連の既定の技法が含まれています。</span><span class="sxs-lookup"><span data-stu-id="88717-116">Usually, feature selection is performed automatically in [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)], and each algorithm has a set of default techniques for intelligently applying feature reduction.</span></span> <span data-ttu-id="88717-117">機能の選択は、常にモデルのトレーニングの前に実行されます。これにより、モデルで使用される可能性の高い属性がデータセット内で自動的に選択されます。</span><span class="sxs-lookup"><span data-stu-id="88717-117">Feature selection is always performed before the model is trained, to automatically choose the attributes in a dataset that are most likely to be used in the model.</span></span> <span data-ttu-id="88717-118">ただし、機能の選択の動作に影響を与えるようにパラメーターを手動で設定することもできます。</span><span class="sxs-lookup"><span data-stu-id="88717-118">However, you can also manually set parameters to influence feature selection behavior.</span></span>  
  
 <span data-ttu-id="88717-119">一般に機能の選択では、各属性のスコアが計算されて、ベスト スコアの属性のみが選択されます。</span><span class="sxs-lookup"><span data-stu-id="88717-119">In general, feature selection works by calculating a score for each attribute, and then selecting only the attributes that have the best scores.</span></span> <span data-ttu-id="88717-120">トップ スコアのしきい値を調整することもできます。</span><span class="sxs-lookup"><span data-stu-id="88717-120">You can also adjust the threshold for the top scores.</span></span> [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] <span data-ttu-id="88717-121">では、これらのスコアを計算するための複数のメソッドを提供しており、モデルに適したメソッドは、次の要因によって異なります。</span><span class="sxs-lookup"><span data-stu-id="88717-121">provides multiple methods for calculating these scores, and the exact method that is applied in any model depends on these factors:</span></span>  
  
-   <span data-ttu-id="88717-122">モデルで使用されるアルゴリズム</span><span class="sxs-lookup"><span data-stu-id="88717-122">The algorithm used in your model</span></span>  
  
-   <span data-ttu-id="88717-123">属性のデータ型</span><span class="sxs-lookup"><span data-stu-id="88717-123">The data type of the attribute</span></span>  
  
-   <span data-ttu-id="88717-124">モデルに設定したパラメーター</span><span class="sxs-lookup"><span data-stu-id="88717-124">Any parameters that you may have set on your model</span></span>  
  
 <span data-ttu-id="88717-125">機能の選択は、列の入力、予測可能な属性、または状態に適用されます。</span><span class="sxs-lookup"><span data-stu-id="88717-125">Feature selection is applied to inputs, predictable attributes, or to states in a column.</span></span> <span data-ttu-id="88717-126">機能の選択のためのスコアリングが完了すると、モデル作成プロセスに含まれ、予測作成に使用できるのは、アルゴリズムが選択した属性と状態のみです。</span><span class="sxs-lookup"><span data-stu-id="88717-126">When scoring for feature selection is complete, only the attributes and states that the algorithm selects are included in the model-building process and can be used for prediction.</span></span> <span data-ttu-id="88717-127">機能の選択のしきい値に合わない予測属性を選択した場合、その属性は予想作成に使用されますが、予測はモデル内に存在する全体統計のみを基礎とします。</span><span class="sxs-lookup"><span data-stu-id="88717-127">If you choose a predictable attribute that does not meet the threshold for feature selection the attribute can still be used for prediction, but the predictions will be based solely on the global statistics that exist in the model.</span></span>  
  
> [!NOTE]  
>  <span data-ttu-id="88717-128">機能の選択が影響するのはモデルで使用される列だけであり、保存されているマイニング構造には影響しません。</span><span class="sxs-lookup"><span data-stu-id="88717-128">Feature selection affects only the columns that are used in the model, and has no effect on storage of the mining structure.</span></span> <span data-ttu-id="88717-129">マイニング モデルから除外された列も、マイニング構造では引き続き使用できます。また、マイニング構造列のデータはキャッシュされます。</span><span class="sxs-lookup"><span data-stu-id="88717-129">The columns that you leave out of the mining model are still available in the structure, and data in the mining structure columns will be cached.</span></span>  
  
### <a name="definition-of-feature-selection-methods"></a><span data-ttu-id="88717-130">機能の選択の方法の定義</span><span class="sxs-lookup"><span data-stu-id="88717-130">Definition of Feature Selection Methods</span></span>  
 <span data-ttu-id="88717-131">機能の選択を実装するには、使用するデータの種類や、分析のために選択するアルゴリズムに応じて、さまざまな方法があります。</span><span class="sxs-lookup"><span data-stu-id="88717-131">There are many ways to implement feature selection, depending on the type of data that you are working with and the algorithm that you choose for analysis.</span></span> <span data-ttu-id="88717-132">SQL Server Analysis Services には、属性のスコアリングのためによく使用される一般的な方法がいくつか用意されています。</span><span class="sxs-lookup"><span data-stu-id="88717-132">SQL Server Analysis Services provides several popular and well-established methods for scoring attributes.</span></span> <span data-ttu-id="88717-133">アルゴリズムまたはデータセットで適用される方法は、データ型および列の使用法に依存します。</span><span class="sxs-lookup"><span data-stu-id="88717-133">The method that is applied in any algorithm or data set depends on the data types, and the column usage.</span></span>  
  
 <span data-ttu-id="88717-134">" *興味深さ* " のスコアは、非バイナリの連続する数値データを含む列の属性を順位付けして並べ替えるために使用されます。</span><span class="sxs-lookup"><span data-stu-id="88717-134">The *interestingness* score is used to rank and sort attributes in columns that contain nonbinary continuous numeric data.</span></span>  
  
 <span data-ttu-id="88717-135">"*Shannon のエントロピ* " のスコア、および 2 つの " *ベイズ* " のスコアは、不連続データおよび分離されたデータを含む列で使用できます。</span><span class="sxs-lookup"><span data-stu-id="88717-135">*Shannon's entropy* and two *Bayesian* scores are available for columns that contain discrete and discretized data.</span></span> <span data-ttu-id="88717-136">ただし、連続した列がモデルに含まれる場合、一貫性を保つために、すべての入力列の評価に興味深さのスコアが使用されます。</span><span class="sxs-lookup"><span data-stu-id="88717-136">However, if the model contains any continuous columns, the interestingness score will be used to assess all input columns, to ensure consistency.</span></span>  
  
 <span data-ttu-id="88717-137">次のセクションでは、機能の選択の各メソッドについて説明します。</span><span class="sxs-lookup"><span data-stu-id="88717-137">The following section describes each method of feature selection.</span></span>  
  
#### <a name="interestingness-score"></a><span data-ttu-id="88717-138">興味深さのスコア</span><span class="sxs-lookup"><span data-stu-id="88717-138">Interestingness score</span></span>  
 <span data-ttu-id="88717-139">機能が興味深いのは、有用な情報を提供する場合です。</span><span class="sxs-lookup"><span data-stu-id="88717-139">A feature is interesting if it tells you some useful piece of information.</span></span> <span data-ttu-id="88717-140">役に立つものの定義はシナリオによって異なるため、データマイニング業界は*興味深さ*を測定するさまざまな方法を開発しました。</span><span class="sxs-lookup"><span data-stu-id="88717-140">Because the definition of what is useful varies depending on the scenario, the data mining industry has developed various ways to measure *interestingness*.</span></span> <span data-ttu-id="88717-141">たとえば、*分野*は外れ値検出では興味深いかもしれませんが、密接に関連する項目や*識別 weight*を区別する機能は、分類の方が興味深いものである可能性があります。</span><span class="sxs-lookup"><span data-stu-id="88717-141">For example, *novelty* might be interesting in outlier detection, but the ability to discriminate between closely related items, or *discriminating weight*, might be more interesting for classification.</span></span>  
  
 <span data-ttu-id="88717-142">SQL Server Analysis Services で使用される興味深さのメジャーは*エントロピに基づい*ています。つまり、ランダムな分布を持つ属性は、エントロピが高く、情報の増加が少なくなります。そのため、このような属性はあまり興味深いものではありません。</span><span class="sxs-lookup"><span data-stu-id="88717-142">The measure of interestingness that is used in SQL Server Analysis Services is *entropy-based*, meaning that attributes with random distributions have higher entropy and lower information gain; therefore, such attributes are less interesting.</span></span> <span data-ttu-id="88717-143">次のようにして、特定の属性のエントロピが他のすべての属性のエントロピと比較されます。</span><span class="sxs-lookup"><span data-stu-id="88717-143">The entropy for any particular attribute is compared to the entropy of all other attributes, as follows:</span></span>  
  
 <span data-ttu-id="88717-144">Interestingness(Attribute) = - (m - Entropy(Attribute)) \* (m - Entropy(Attribute))</span><span class="sxs-lookup"><span data-stu-id="88717-144">Interestingness(Attribute) = - (m - Entropy(Attribute)) \* (m - Entropy(Attribute))</span></span>  
  
 <span data-ttu-id="88717-145">中心エントロピ (m) は、機能セット全体のエントロピを表します。</span><span class="sxs-lookup"><span data-stu-id="88717-145">Central entropy, or m, means the entropy of the entire feature set.</span></span> <span data-ttu-id="88717-146">対象となる属性のエントロピを中心エントロピから差し引くことにより、その属性が提供する情報の量を評価できます。</span><span class="sxs-lookup"><span data-stu-id="88717-146">By subtracting the entropy of the target attribute from the central entropy, you can assess how much information the attribute provides.</span></span>  
  
 <span data-ttu-id="88717-147">列に非バイナリの連続する数値データが含まれている場合は、常にこのスコアが既定で使用されます。</span><span class="sxs-lookup"><span data-stu-id="88717-147">This score is used by default whenever the column contains nonbinary continuous numeric data.</span></span>  
  
#### <a name="shannons-entropy"></a><span data-ttu-id="88717-148">Shannon のエントロピー</span><span class="sxs-lookup"><span data-stu-id="88717-148">Shannon's Entropy</span></span>  
 <span data-ttu-id="88717-149">Shannon のエントロピは、特定の結果に対する確率変数の不確かさを測定します。</span><span class="sxs-lookup"><span data-stu-id="88717-149">Shannon's entropy measures the uncertainty of a random variable for a particular outcome.</span></span> <span data-ttu-id="88717-150">たとえばコイン投げのエントロピは、コインが表になる確率の関数として表すことができます。</span><span class="sxs-lookup"><span data-stu-id="88717-150">For example, the entropy of a coin toss can be represented as a function of the probability of it coming up heads.</span></span>  
  
 <span data-ttu-id="88717-151">Analysis Services では、次の数式を使用して Shannon のエントロピを計算します。</span><span class="sxs-lookup"><span data-stu-id="88717-151">Analysis Services uses the following formula to calculate Shannon's entropy:</span></span>  
  
 <span data-ttu-id="88717-152">H(X) = -  P(xi) log(P(xi))</span><span class="sxs-lookup"><span data-stu-id="88717-152">H(X) = -∑ P(xi) log(P(xi))</span></span>  
  
 <span data-ttu-id="88717-153">このスコアリング方法は、不連続属性と分離された属性で使用できます。</span><span class="sxs-lookup"><span data-stu-id="88717-153">This scoring method is available for discrete and discretized attributes.</span></span>  
  
#### <a name="bayesian-with-k2-prior"></a><span data-ttu-id="88717-154">K2 事前分布を指定したベイズ定理</span><span class="sxs-lookup"><span data-stu-id="88717-154">Bayesian with K2 Prior</span></span>  
 <span data-ttu-id="88717-155">Analysis Services には、ベイジアン ネットワークに基づく 2 つの機能選択スコアが用意されています。</span><span class="sxs-lookup"><span data-stu-id="88717-155">Analysis Services provides two feature selection scores that are based on Bayesian networks.</span></span> <span data-ttu-id="88717-156">ベイジアン ネットワークとは、状態および状態間の遷移の " *有向* " または " *非循環* " のグラフ (常に現在の状態より前にある状態と後にある状態がある、繰り返し (ループ) を含まないグラフ) です。</span><span class="sxs-lookup"><span data-stu-id="88717-156">A Bayesian network is a *directed* or *acyclic* graph of states and transitions between states, meaning that some states are always prior to the current state, some states are posterior, and the graph does not repeat or loop.</span></span> <span data-ttu-id="88717-157">定義上、ベイジアン ネットワークでは事前知識を使用できます。</span><span class="sxs-lookup"><span data-stu-id="88717-157">By definition, Bayesian networks allow the use of prior knowledge.</span></span> <span data-ttu-id="88717-158">ただし、前の状態のうちのどれを使用して後の状態の確率を計算するかという問題が、アルゴリズムのデザイン、パフォーマンス、および精度にとって重要になります。</span><span class="sxs-lookup"><span data-stu-id="88717-158">However, the question of which prior states to use in calculating probabilities of later states is important for algorithm design, performance, and accuracy.</span></span>  
  
 <span data-ttu-id="88717-159">ベイジアン ネットワークの学習のための K2 アルゴリズムは、Cooper と Herskovits によって開発されたもので、データ マイニングでよく使用されます。</span><span class="sxs-lookup"><span data-stu-id="88717-159">The K2 algorithm for learning from a Bayesian network was developed by Cooper and Herskovits and is often used in data mining.</span></span> <span data-ttu-id="88717-160">K2 アルゴリズムは拡張可能で、複数の変数を分析できますが、入力として使用する変数の順序付けが必要とされます。</span><span class="sxs-lookup"><span data-stu-id="88717-160">It is scalable and can analyze multiple variables, but requires ordering on variables used as input.</span></span> <span data-ttu-id="88717-161">詳細については、「 [ベイジアン ネットワークの学習: 知識と統計データの組み合わせ](https://go.microsoft.com/fwlink/?LinkId=105885) 」(Chickering、Geiger、および Heckerman) を参照してください。</span><span class="sxs-lookup"><span data-stu-id="88717-161">For more information, see [Learning Bayesian Networks](https://go.microsoft.com/fwlink/?LinkId=105885) by Chickering, Geiger, and Heckerman.</span></span>  
  
 <span data-ttu-id="88717-162">このスコアリング方法は、不連続属性と分離された属性で使用できます。</span><span class="sxs-lookup"><span data-stu-id="88717-162">This scoring method is available for discrete and discretized attributes.</span></span>  
  
#### <a name="bayesian-dirichlet-equivalent-with-uniform-prior"></a><span data-ttu-id="88717-163">均一な事前分布を指定したベイズ ディリクレ等式</span><span class="sxs-lookup"><span data-stu-id="88717-163">Bayesian Dirichlet Equivalent with Uniform Prior</span></span>  
 <span data-ttu-id="88717-164">ベイズ ディリクレ等式 (BDE) スコアも、与えられたデータセットについて、ベイズ解析を使用してネットワークを評価します。</span><span class="sxs-lookup"><span data-stu-id="88717-164">The Bayesian Dirichlet Equivalent (BDE) score also uses Bayesian analysis to evaluate a network given a dataset.</span></span> <span data-ttu-id="88717-165">BDE のスコアリング方法は Heckerman によって開発されたもので、Cooper と Herskovits によって開発された BD メトリックに基づいています。</span><span class="sxs-lookup"><span data-stu-id="88717-165">The BDE scoring method was developed by Heckerman and is based on the BD metric developed by Cooper and Herskovits.</span></span> <span data-ttu-id="88717-166">ディリクレ分布は、ネットワークの各変数の条件付き確率を表す多項分布で、学習に役立つ数多くの特性があります。</span><span class="sxs-lookup"><span data-stu-id="88717-166">The Dirichlet distribution is a multinomial distribution that describes the conditional probability of each variable in the network, and has many properties that are useful for learning.</span></span>  
  
 <span data-ttu-id="88717-167">均一な事前分布を指定したベイズ ディリクレ等式 (BDEU) の方法では、数学定数を使用して事前状態の固定分布 (均一な分布) が作成されるディリクレ分布の特殊なケースが想定されています。</span><span class="sxs-lookup"><span data-stu-id="88717-167">The Bayesian Dirichlet Equivalent with Uniform Prior (BDEU) method assumes a special case of the Dirichlet distribution, in which a mathematical constant is used to create a fixed or uniform distribution of prior states.</span></span> <span data-ttu-id="88717-168">また尤度等価も想定されているため、データで等価な構造が区別されることを期待できません。</span><span class="sxs-lookup"><span data-stu-id="88717-168">The BDE score also assumes likelihood equivalence, which means that the data cannot be expected to discriminate equivalent structures.</span></span> <span data-ttu-id="88717-169">つまり、If A Then B のスコアが If B Then A のスコアと同じ場合、そのデータに基づいて構造を区別することはできず、因果関係を推論できません。</span><span class="sxs-lookup"><span data-stu-id="88717-169">In other words, if the score for If A Then B is the same as the score for If B Then A, the structures cannot be distinguished based on the data, and causation cannot be inferred.</span></span>  
  
 <span data-ttu-id="88717-170">ベイジアン ネットワークの詳細およびこれらのスコアリング方法の実装の詳細については、「 [ベイジアン ネットワークの学習 : 知識と統計データの組み合わせ](https://go.microsoft.com/fwlink/?LinkId=105885)」を参照してください。</span><span class="sxs-lookup"><span data-stu-id="88717-170">For more information about Bayesian networks and the implementation of these scoring methods, see [Learning Bayesian Networks](https://go.microsoft.com/fwlink/?LinkId=105885).</span></span>  
  
### <a name="feature-selection-methods-used-by-analysis-services-algorithms"></a><span data-ttu-id="88717-171">Analysis Services のアルゴリズムで使用される機能の選択の方法</span><span class="sxs-lookup"><span data-stu-id="88717-171">Feature Selection Methods used by Analysis Services Algorithms</span></span>  
 <span data-ttu-id="88717-172">次の表は、機能の選択をサポートするアルゴリズム、そのアルゴリズムによって使用される機能の選択の方法、および機能の選択の動作を制御するために設定するパラメーターの一覧です。</span><span class="sxs-lookup"><span data-stu-id="88717-172">The following table lists the algorithms that support feature selection, the feature selection methods used by the algorithm, and the parameters that you set to control feature selection behavior:</span></span>  
  
|<span data-ttu-id="88717-173">アルゴリズム</span><span class="sxs-lookup"><span data-stu-id="88717-173">Algorithm</span></span>|<span data-ttu-id="88717-174">分析の方法</span><span class="sxs-lookup"><span data-stu-id="88717-174">Method of analysis</span></span>|<span data-ttu-id="88717-175">説明</span><span class="sxs-lookup"><span data-stu-id="88717-175">Comments</span></span>|  
|---------------|------------------------|--------------|  
|<span data-ttu-id="88717-176">Naive Bayes</span><span class="sxs-lookup"><span data-stu-id="88717-176">Naive Bayes</span></span>|<span data-ttu-id="88717-177">Shannon のエントロピー</span><span class="sxs-lookup"><span data-stu-id="88717-177">Shannon's Entropy</span></span><br /><br /> <span data-ttu-id="88717-178">K2 事前分布を指定したベイズ定理</span><span class="sxs-lookup"><span data-stu-id="88717-178">Bayesian with K2 Prior</span></span><br /><br /> <span data-ttu-id="88717-179">均一な事前分布を指定したベイズ ディリクレ等式 (既定値)</span><span class="sxs-lookup"><span data-stu-id="88717-179">Bayesian Dirichlet with uniform prior (default)</span></span>|<span data-ttu-id="88717-180">Microsoft Naïve Bayes アルゴリズムで使用できる属性は、不連続属性と分離された属性だけです。したがって、興味深さのスコアは使用できません。</span><span class="sxs-lookup"><span data-stu-id="88717-180">The Microsoft Naïve Bayes algorithm accepts only discrete or discretized attributes; therefore, it cannot use the interestingness score.</span></span><br /><br /> <span data-ttu-id="88717-181">このアルゴリズムの詳細については、「 [Microsoft Naive Bayes アルゴリズム テクニカル リファレンス](microsoft-naive-bayes-algorithm-technical-reference.md)」を参照してください。</span><span class="sxs-lookup"><span data-stu-id="88717-181">For more information about this algorithm, see [Microsoft Naive Bayes Algorithm Technical Reference](microsoft-naive-bayes-algorithm-technical-reference.md).</span></span>|  
|<span data-ttu-id="88717-182">デシジョン ツリー</span><span class="sxs-lookup"><span data-stu-id="88717-182">Decision trees</span></span>|<span data-ttu-id="88717-183">興味深さのスコア</span><span class="sxs-lookup"><span data-stu-id="88717-183">Interestingness score</span></span><br /><br /> <span data-ttu-id="88717-184">Shannon のエントロピー</span><span class="sxs-lookup"><span data-stu-id="88717-184">Shannon's Entropy</span></span><br /><br /> <span data-ttu-id="88717-185">K2 事前分布を指定したベイズ定理</span><span class="sxs-lookup"><span data-stu-id="88717-185">Bayesian with K2 Prior</span></span><br /><br /> <span data-ttu-id="88717-186">均一な事前分布を指定したベイズ ディリクレ等式 (既定値)</span><span class="sxs-lookup"><span data-stu-id="88717-186">Bayesian Dirichlet with uniform prior (default)</span></span>|<span data-ttu-id="88717-187">非バイナリの連続する値を含む列がある場合は、一貫性を保つため、すべての列に対して興味深さのスコアが使用されます。</span><span class="sxs-lookup"><span data-stu-id="88717-187">If any columns contain non-binary continuous values, the interestingness score is used for all columns, to ensure consistency.</span></span> <span data-ttu-id="88717-188">それ以外の場合は、既定の機能の選択の方法か、モデルを作成したときに指定した方法が使用されます。</span><span class="sxs-lookup"><span data-stu-id="88717-188">Otherwise, the default feature selection method is used, or the method that you specified when you created the model.</span></span><br /><br /> <span data-ttu-id="88717-189">このアルゴリズムの詳細については、「 [Microsoft デシジョン ツリー アルゴリズム テクニカル リファレンス](microsoft-decision-trees-algorithm-technical-reference.md)」を参照してください。</span><span class="sxs-lookup"><span data-stu-id="88717-189">For more information about this algorithm, see [Microsoft Decision Trees Algorithm Technical Reference](microsoft-decision-trees-algorithm-technical-reference.md).</span></span>|  
|<span data-ttu-id="88717-190">ニューラル ネットワーク</span><span class="sxs-lookup"><span data-stu-id="88717-190">Neural network</span></span>|<span data-ttu-id="88717-191">興味深さのスコア</span><span class="sxs-lookup"><span data-stu-id="88717-191">Interestingness score</span></span><br /><br /> <span data-ttu-id="88717-192">Shannon のエントロピー</span><span class="sxs-lookup"><span data-stu-id="88717-192">Shannon's Entropy</span></span><br /><br /> <span data-ttu-id="88717-193">K2 事前分布を指定したベイズ定理</span><span class="sxs-lookup"><span data-stu-id="88717-193">Bayesian with K2 Prior</span></span><br /><br /> <span data-ttu-id="88717-194">均一な事前分布を指定したベイズ ディリクレ等式 (既定値)</span><span class="sxs-lookup"><span data-stu-id="88717-194">Bayesian Dirichlet with uniform prior (default)</span></span>|<span data-ttu-id="88717-195">Microsoft ニューラル ネットワーク アルゴリズムでは、データに連続列が含まれている限り、ベイズおよびエントロピに基づく方法の両方を使用できます。</span><span class="sxs-lookup"><span data-stu-id="88717-195">The Microsoft Neural Networks algorithm can use both Bayesian and entropy-based methods, as long as the data contains continuous columns.</span></span><br /><br /> <span data-ttu-id="88717-196">このアルゴリズムの詳細については、「 [Microsoft ニューラル ネットワーク アルゴリズム テクニカル リファレンス](microsoft-neural-network-algorithm-technical-reference.md)」を参照してください。</span><span class="sxs-lookup"><span data-stu-id="88717-196">For more information about this algorithm, see [Microsoft Neural Network Algorithm Technical Reference](microsoft-neural-network-algorithm-technical-reference.md).</span></span>|  
|<span data-ttu-id="88717-197">Logistic regression (ロジスティック回帰)</span><span class="sxs-lookup"><span data-stu-id="88717-197">Logistic regression</span></span>|<span data-ttu-id="88717-198">興味深さのスコア</span><span class="sxs-lookup"><span data-stu-id="88717-198">Interestingness score</span></span><br /><br /> <span data-ttu-id="88717-199">Shannon のエントロピー</span><span class="sxs-lookup"><span data-stu-id="88717-199">Shannon's Entropy</span></span><br /><br /> <span data-ttu-id="88717-200">K2 事前分布を指定したベイズ定理</span><span class="sxs-lookup"><span data-stu-id="88717-200">Bayesian with K2 Prior</span></span><br /><br /> <span data-ttu-id="88717-201">均一な事前分布を指定したベイズ ディリクレ等式 (既定値)</span><span class="sxs-lookup"><span data-stu-id="88717-201">Bayesian Dirichlet with uniform prior (default)</span></span>|<span data-ttu-id="88717-202">Microsoft ロジスティック回帰アルゴリズムは Microsoft ニューラル ネットワーク アルゴリズムに基づいていますが、ロジスティック回帰モデルをカスタマイズして機能の選択動作を制御することはできません。したがって、機能の選択では、常に属性に最も適した方法が既定で使用されます。</span><span class="sxs-lookup"><span data-stu-id="88717-202">Although the Microsoft Logistic Regression algorithm is based on the Microsoft Neural Network algorithm, you cannot customize logistic regression models to control feature selection behavior; therefore, feature selection always default to the method that is most appropriate for the attribute.</span></span><br /><br /> <span data-ttu-id="88717-203">すべての属性が不連続属性または分離された属性の場合は、既定値は BDEU です。</span><span class="sxs-lookup"><span data-stu-id="88717-203">If all attributes are discrete or discretized, the default is BDEU.</span></span><br /><br /> <span data-ttu-id="88717-204">このアルゴリズムの詳細については、「 [Microsoft ロジスティック回帰アルゴリズム テクニカル リファレンス](microsoft-logistic-regression-algorithm-technical-reference.md)」を参照してください。</span><span class="sxs-lookup"><span data-stu-id="88717-204">For more information about this algorithm, see [Microsoft Logistic Regression Algorithm Technical Reference](microsoft-logistic-regression-algorithm-technical-reference.md).</span></span>|  
|<span data-ttu-id="88717-205">クラスタリング</span><span class="sxs-lookup"><span data-stu-id="88717-205">Clustering</span></span>|<span data-ttu-id="88717-206">興味深さのスコア</span><span class="sxs-lookup"><span data-stu-id="88717-206">Interestingness score</span></span>|<span data-ttu-id="88717-207">Microsoft クラスタリング アルゴリズムでは、不連続なデータまたは分離されたデータを使用できます。</span><span class="sxs-lookup"><span data-stu-id="88717-207">The Microsoft Clustering algorithm can use discrete or discretized data.</span></span> <span data-ttu-id="88717-208">ただし、各属性のスコアは距離として計算され、連続する数値として表現されるため、興味深さのスコアを使用する必要があります。</span><span class="sxs-lookup"><span data-stu-id="88717-208">However, because the score of each attribute is calculated as a distance and is represented as a continuous number, the interestingness score must be used.</span></span><br /><br /> <span data-ttu-id="88717-209">このアルゴリズムの詳細については、「 [Microsoft クラスタリング アルゴリズム テクニカル リファレンス](microsoft-clustering-algorithm-technical-reference.md)」を参照してください。</span><span class="sxs-lookup"><span data-stu-id="88717-209">For more information about this algorithm, see [Microsoft Clustering Algorithm Technical Reference](microsoft-clustering-algorithm-technical-reference.md).</span></span>|  
|<span data-ttu-id="88717-210">Linear regression (線形回帰)</span><span class="sxs-lookup"><span data-stu-id="88717-210">Linear regression</span></span>|<span data-ttu-id="88717-211">興味深さのスコア</span><span class="sxs-lookup"><span data-stu-id="88717-211">Interestingness score</span></span>|<span data-ttu-id="88717-212">Microsoft 線形回帰アルゴリズムでは、連続列のみをサポートするため、使用できるのは興味深さのスコアだけです。</span><span class="sxs-lookup"><span data-stu-id="88717-212">The Microsoft Linear Regression algorithm can only use the interestingness score, because it only supports continuous columns.</span></span><br /><br /> <span data-ttu-id="88717-213">このアルゴリズムの詳細については、「 [Microsoft 線形回帰アルゴリズム テクニカル リファレンス](microsoft-linear-regression-algorithm-technical-reference.md)」を参照してください。</span><span class="sxs-lookup"><span data-stu-id="88717-213">For more information about this algorithm, see [Microsoft Linear Regression Algorithm Technical Reference](microsoft-linear-regression-algorithm-technical-reference.md).</span></span>|  
|<span data-ttu-id="88717-214">相関ルール</span><span class="sxs-lookup"><span data-stu-id="88717-214">Association rules</span></span><br /><br /> <span data-ttu-id="88717-215">シーケンス クラスター</span><span class="sxs-lookup"><span data-stu-id="88717-215">Sequence clustering</span></span>|<span data-ttu-id="88717-216">使用されていない</span><span class="sxs-lookup"><span data-stu-id="88717-216">Not used</span></span>|<span data-ttu-id="88717-217">これらのアルゴリズムでは、機能の選択は実行されません。</span><span class="sxs-lookup"><span data-stu-id="88717-217">Feature selection is not invoked with these algorithms.</span></span><br /><br /> <span data-ttu-id="88717-218">ただし、必要に応じてパラメーター MINIMUM_SUPPORT および MINIMUM_PROBABILIITY の値を設定することによって、アルゴリズムの動作を制御し、入力データのサイズを小さくすることができます。</span><span class="sxs-lookup"><span data-stu-id="88717-218">However, you can control the behavior of the algorithm and reduce the size of input data if necessary by setting the value of the parameters MINIMUM_SUPPORT and MINIMUM_PROBABILIITY.</span></span><br /><br /> <span data-ttu-id="88717-219">詳細については、「 [Microsoft アソシエーション アルゴリズム テクニカル リファレンス](microsoft-association-algorithm-technical-reference.md) 」および「 [Microsoft シーケンス クラスタリング アルゴリズム テクニカル リファレンス](microsoft-sequence-clustering-algorithm-technical-reference.md)」を参照してください。</span><span class="sxs-lookup"><span data-stu-id="88717-219">For more information, see [Microsoft Association Algorithm Technical Reference](microsoft-association-algorithm-technical-reference.md) and [Microsoft Sequence Clustering Algorithm Technical Reference](microsoft-sequence-clustering-algorithm-technical-reference.md).</span></span>|  
|<span data-ttu-id="88717-220">タイム シリーズ</span><span class="sxs-lookup"><span data-stu-id="88717-220">Time series</span></span>|<span data-ttu-id="88717-221">使用されていない</span><span class="sxs-lookup"><span data-stu-id="88717-221">Not used</span></span>|<span data-ttu-id="88717-222">機能の選択は、時系列モデルには適用されません。</span><span class="sxs-lookup"><span data-stu-id="88717-222">Feature selection does not apply to time series models.</span></span><br /><br /> <span data-ttu-id="88717-223">このアルゴリズムの詳細については、「 [Microsoft Time Series アルゴリズム テクニカル リファレンス](microsoft-time-series-algorithm-technical-reference.md)」を参照してください。</span><span class="sxs-lookup"><span data-stu-id="88717-223">For more information about this algorithm, see [Microsoft Time Series Algorithm Technical Reference](microsoft-time-series-algorithm-technical-reference.md).</span></span>|  
  
## <a name="feature-selection-parameters"></a><span data-ttu-id="88717-224">機能の選択のパラメーター</span><span class="sxs-lookup"><span data-stu-id="88717-224">Feature Selection Parameters</span></span>  
 <span data-ttu-id="88717-225">機能の選択をサポートするアルゴリズムでは、以下のパラメーターを使用して、機能の選択をいつオンにするかを制御できます。</span><span class="sxs-lookup"><span data-stu-id="88717-225">In algorithms that support feature selection, you can control when feature selection is turned on by using the following parameters.</span></span> <span data-ttu-id="88717-226">各アルゴリズムには、許可される入力数の既定値がありますが、その既定値をオーバーライドして属性の数を指定できます。</span><span class="sxs-lookup"><span data-stu-id="88717-226">Each algorithm has a default value for the number of inputs that are allowed, but you can override this default and specify the number of attributes.</span></span> <span data-ttu-id="88717-227">このセクションは、機能の選択を管理するために提供されるパラメーターを示します。</span><span class="sxs-lookup"><span data-stu-id="88717-227">This section lists the parameters that are provided for managing feature selection.</span></span>  
  
#### <a name="maximum_input_attributes"></a><span data-ttu-id="88717-228">MAXIMUM_INPUT_ATTRIBUTES</span><span class="sxs-lookup"><span data-stu-id="88717-228">MAXIMUM_INPUT_ATTRIBUTES</span></span>  
 <span data-ttu-id="88717-229">*MAXIMUM_INPUT_ATTRIBUTES* パラメーターで指定した数より多い列がモデルにある場合、アルゴリズムでは、計算により無意味であると判断されたすべての列が無視されます。</span><span class="sxs-lookup"><span data-stu-id="88717-229">If a model contains more columns than the number that is specified in the *MAXIMUM_INPUT_ATTRIBUTES* parameter, the algorithm ignores any columns that it calculates to be uninteresting.</span></span>  
  
#### <a name="maximum_output_attributes"></a><span data-ttu-id="88717-230">MAXIMUM_OUTPUT_ATTRIBUTES</span><span class="sxs-lookup"><span data-stu-id="88717-230">MAXIMUM_OUTPUT_ATTRIBUTES</span></span>  
 <span data-ttu-id="88717-231">同様に、 *MAXIMUM_OUTPUT_ATTRIBUTES* パラメーターで指定した数より多い予測可能列がモデルにある場合、アルゴリズムでは、計算により無意味であると判断されたすべての列が無視されます。</span><span class="sxs-lookup"><span data-stu-id="88717-231">Similarly, if a model contains more predictable columns than the number that is specified in the *MAXIMUM_OUTPUT_ATTRIBUTES* parameter, the algorithm ignores any columns that it calculates to be uninteresting.</span></span>  
  
#### <a name="maximum_states"></a><span data-ttu-id="88717-232">MAXIMUM_STATES</span><span class="sxs-lookup"><span data-stu-id="88717-232">MAXIMUM_STATES</span></span>  
 <span data-ttu-id="88717-233">モデルに *MAXIMUM_STATES* パラメーターで指定された数より多いケースがある場合、最も一般的でない状態はグループ化され、無視されます。</span><span class="sxs-lookup"><span data-stu-id="88717-233">If a model contains more cases than are specified in the *MAXIMUM_STATES* parameter, the least popular states are grouped together and treated as missing.</span></span> <span data-ttu-id="88717-234">これらのパラメーターのいずれかが 0 に設定されている場合、機能の選択はオフになり、処理時間とパフォーマンスに影響を及ぼします。</span><span class="sxs-lookup"><span data-stu-id="88717-234">If any one of these parameters is set to 0, feature selection is turned off, affecting processing time and performance.</span></span>  
  
 <span data-ttu-id="88717-235">機能の選択のこれらのメソッドに加え、モデルの " *モデリング フラグ* "、または構造の " *ディストリビューション フラグ* " を設定すると、アルゴリズム機能を改善して重要な属性を識別したり昇格させたりすることができます。</span><span class="sxs-lookup"><span data-stu-id="88717-235">In addition to these methods for feature selection, you can improve the ability of the algorithm to identify or promote meaningful attributes by setting *modeling flags* on the model or by setting *distribution flags* on the structure.</span></span> <span data-ttu-id="88717-236">これらの概念の詳細については、「[モデリング フラグ &#40;データ マイニング&#41;](modeling-flags-data-mining.md)」および「[列の分布 &#40;データ マイニング&#41;](column-distributions-data-mining.md)」を参照してください。</span><span class="sxs-lookup"><span data-stu-id="88717-236">For more information about these concepts, see [Modeling Flags &#40;Data Mining&#41;](modeling-flags-data-mining.md) and [Column Distributions &#40;Data Mining&#41;](column-distributions-data-mining.md).</span></span>  
  
## <a name="see-also"></a><span data-ttu-id="88717-237">参照</span><span class="sxs-lookup"><span data-stu-id="88717-237">See Also</span></span>  
 [<span data-ttu-id="88717-238">マイニング モデルとマイニング構造のカスタマイズ</span><span class="sxs-lookup"><span data-stu-id="88717-238">Customize Mining Models and Structure</span></span>](customize-mining-models-and-structure.md)  
  
  
