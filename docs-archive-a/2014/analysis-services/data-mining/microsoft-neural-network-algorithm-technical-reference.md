---
title: Microsoft ニューラルネットワークアルゴリズムテクニカルリファレンス |Microsoft Docs
ms.custom: ''
ms.date: 06/13/2017
ms.prod: sql-server-2014
ms.reviewer: ''
ms.technology: analysis-services
ms.topic: conceptual
helpviewer_keywords:
- HIDDEN_NODE_RATIO parameter
- MAXIMUM_INPUT_ATTRIBUTES parameter
- HOLDOUT_PERCENTAGE parameter
- neural network algorithms [Analysis Services]
- output layer [Data Mining]
- neural networks
- MAXIMUM_OUTPUT_ATTRIBUTES parameter
- MAXIMUM_STATES parameter
- SAMPLE_SIZE parameter
- hidden layer
- hidden neurons
- input layer [Data Mining]
- activation function [Data Mining]
- Back-Propagated Delta Rule network
- neural network model [Analysis Services]
- coding [Data Mining]
- HOLDOUT_SEED parameter
ms.assetid: b8fac409-e3c0-4216-b032-364f8ea51095
author: minewiskan
ms.author: owend
ms.openlocfilehash: 3c36fd9f3446ddf36da9af7ce58259edbe84c8cf
ms.sourcegitcommit: ad4d92dce894592a259721a1571b1d8736abacdb
ms.translationtype: MT
ms.contentlocale: ja-JP
ms.lasthandoff: 08/04/2020
ms.locfileid: "87631356"
---
# <a name="microsoft-neural-network-algorithm-technical-reference"></a><span data-ttu-id="699d7-102">Microsoft Neural Network Algorithm Technical Reference</span><span class="sxs-lookup"><span data-stu-id="699d7-102">Microsoft Neural Network Algorithm Technical Reference</span></span>
  <span data-ttu-id="699d7-103">[!INCLUDE[msCoName](../../includes/msconame-md.md)] ニューラル ネットワークでは、最大 3 層のニューロンまたは *パーセプトロン* で構成される *多層パーセプトロン*ネットワーク ( *バックプロパゲーション デルタ ルール ネットワーク*とも呼ばれる) を使用します。</span><span class="sxs-lookup"><span data-stu-id="699d7-103">The [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network uses a *Multilayer Perceptron* network, also called a *Back-Propagated Delta Rule network*, composed of up to three layers of neurons, or *perceptrons*.</span></span> <span data-ttu-id="699d7-104">これらの層は、入力層、オプションの非表示層、および出力層です。</span><span class="sxs-lookup"><span data-stu-id="699d7-104">These layers are an input layer, an optional hidden layer, and an output layer.</span></span>  
  
 <span data-ttu-id="699d7-105">多層パーセプトロン ニューラル ネットワークの詳細については、このマニュアルでは扱いません。</span><span class="sxs-lookup"><span data-stu-id="699d7-105">A detailed discussion of Multilayer Perceptron neural networks is outside the scope of this documentation.</span></span> <span data-ttu-id="699d7-106">このトピックでは、入力値および出力値を正規化するために使用する方法や属性のカーディナリティを減らすために使用する機能選択方法など、アルゴリズムの基本的な実装について説明します。</span><span class="sxs-lookup"><span data-stu-id="699d7-106">This topic explains the basic implementation of the algorithm, including the method used to normalize input and output values, and feature selection methods used to reduce attribute cardinality.</span></span> <span data-ttu-id="699d7-107">このトピックでは、アルゴリズムの動作をカスタマイズするために使用できるパラメーターおよびその他の設定について説明します。モデルのクエリに関する追加情報へのリンクも示します。</span><span class="sxs-lookup"><span data-stu-id="699d7-107">This topic describes the parameters and other settings that can be used to customize the behavior of the algorithm, and provides links to additional information about querying the model.</span></span>  
  
## <a name="implementation-of-the-microsoft-neural-network-algorithm"></a><span data-ttu-id="699d7-108">Microsoft ニューラル ネットワーク アルゴリズムの実装</span><span class="sxs-lookup"><span data-stu-id="699d7-108">Implementation of the Microsoft Neural Network Algorithm</span></span>  
 <span data-ttu-id="699d7-109">多層パーセプトロン ニューラル ネットワークでは、各ニューロンは 1 つまたは複数の入力を受け取り、1 つまたは複数の同一の出力を生成します。</span><span class="sxs-lookup"><span data-stu-id="699d7-109">In a Multilayer Perceptron neural network, each neuron receives one or more inputs and produces one or more identical outputs.</span></span> <span data-ttu-id="699d7-110">各出力は、ニューロンへの入力の合計の単純な非線形関数です。</span><span class="sxs-lookup"><span data-stu-id="699d7-110">Each output is a simple non-linear function of the sum of the inputs to the neuron.</span></span> <span data-ttu-id="699d7-111">入力は入力層のノードから非表示層のノードに送られ、次に非表示層から出力層に渡されます。層内のニューロン間は接続されていません。</span><span class="sxs-lookup"><span data-stu-id="699d7-111">Inputs pass forward from nodes in the input layer to nodes in the hidden layer, and then pass from the hidden layer to the output layer; there are no connections between neurons within a layer.</span></span> <span data-ttu-id="699d7-112">ロジスティック回帰モデルのように、非表示層が含まれていない場合、入力は入力層のノードから出力層のノードに直接渡されます。</span><span class="sxs-lookup"><span data-stu-id="699d7-112">If no hidden layer is included, as in a logistic regression model, inputs pass forward directly from nodes in the input layer to nodes in the output layer.</span></span>  
  
 <span data-ttu-id="699d7-113">[!INCLUDE[msCoName](../../includes/msconame-md.md)] ニューラル ネットワーク アルゴリズムで作成されるニューラル ネットワークには、次の 3 種類のニューロンがあります。</span><span class="sxs-lookup"><span data-stu-id="699d7-113">There are three types of neurons in a neural network that is created with the [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network algorithm:</span></span>  
  
-   `Input neurons`  
  
 <span data-ttu-id="699d7-114">入力ニューロンは、データ マイニング モデルの入力属性値を指定します。</span><span class="sxs-lookup"><span data-stu-id="699d7-114">Input neurons provide input attribute values for the data mining model.</span></span> <span data-ttu-id="699d7-115">不連続の入力属性の場合、入力ニューロンは通常、入力属性からの 1 つの状態を表します。</span><span class="sxs-lookup"><span data-stu-id="699d7-115">For discrete input attributes, an input neuron typically represents a single state from the input attribute.</span></span> <span data-ttu-id="699d7-116">この状態には不足値が含まれます。状態が不足値になるのは、属性のトレーニング データに NULL が含まれている場合です。</span><span class="sxs-lookup"><span data-stu-id="699d7-116">This includes missing values, if the training data contains nulls for that attribute.</span></span> <span data-ttu-id="699d7-117">3 つ以上の状態を持つ不連続の入力属性では、各状態の入力ニューロンが 1 つずつ生成されます。また、トレーニング データに NULL が含まれている場合は、存在しない状態の入力ニューロンが 1 つ生成されます。</span><span class="sxs-lookup"><span data-stu-id="699d7-117">A discrete input attribute that has more than two states generates one input neuron for each state, and one input neuron for a missing state, if there are any nulls in the training data.</span></span> <span data-ttu-id="699d7-118">連続する入力属性では、存在しない状態のニューロンと、連続する属性自体の値のニューロンという 2 つの入力ニューロンが生成されます。</span><span class="sxs-lookup"><span data-stu-id="699d7-118">A continuous input attribute generates two input neurons: one neuron for a missing state, and one neuron for the value of the continuous attribute itself.</span></span> <span data-ttu-id="699d7-119">入力ニューロンは、1 つまたは複数の非表示ニューロンの入力になります。</span><span class="sxs-lookup"><span data-stu-id="699d7-119">Input neurons provide inputs to one or more hidden neurons.</span></span>  
  
-   `Hidden neurons`  
  
 <span data-ttu-id="699d7-120">非表示ニューロンは入力ニューロンから入力を受け取り、出力ニューロンに出力を渡します。</span><span class="sxs-lookup"><span data-stu-id="699d7-120">Hidden neurons receive inputs from input neurons and provide outputs to output neurons.</span></span>  
  
-   `Output neurons`  
  
 <span data-ttu-id="699d7-121">出力ニューロンは、データ マイニング モデルの予測可能属性値を表します。</span><span class="sxs-lookup"><span data-stu-id="699d7-121">Output neurons represent predictable attribute values for the data mining model.</span></span> <span data-ttu-id="699d7-122">不連続の入力属性の場合、出力ニューロンは通常、欠落値などの予測可能属性の 1 つの予測状態を表します。</span><span class="sxs-lookup"><span data-stu-id="699d7-122">For discrete input attributes, an output neuron typically represents a single predicted state for a predictable attribute, including missing values.</span></span> <span data-ttu-id="699d7-123">たとえば、バイナリの予測可能属性では、存在しない状態または存在する状態を表す 1 つの出力ノードが生成され、その属性の値が存在するかどうかが示されます。</span><span class="sxs-lookup"><span data-stu-id="699d7-123">For example, a binary predictable attribute produces one output node that describes a missing or existing state, to indicate whether a value exists for that attribute.</span></span> <span data-ttu-id="699d7-124">予測可能属性として使用されるブール型の列では、true 値のニューロン、false 値のニューロン、および存在しない状態または存在する状態のニューロンという 3 つの出力ニューロンが生成されます。</span><span class="sxs-lookup"><span data-stu-id="699d7-124">A Boolean column that is used as a predictable attribute generates three output neurons: one neuron for a true value, one neuron for a false value, and one neuron for a missing or existing state.</span></span> <span data-ttu-id="699d7-125">3 つ以上の状態を持つ不連続の予測可能属性では、各状態の出力ニューロンと、存在しない状態または存在する状態の出力ニューロンが 1 つずつ生成されます。</span><span class="sxs-lookup"><span data-stu-id="699d7-125">A discrete predictable attribute that has more than two states generates one output neuron for each state, and one output neuron for a missing or existing state.</span></span> <span data-ttu-id="699d7-126">連続する予測可能な列では、存在しない状態または存在する状態のニューロンと、連続する列自体の値のニューロンという 2 つの出力ニューロンが生成されます。</span><span class="sxs-lookup"><span data-stu-id="699d7-126">Continuous predictable columns generate two output neurons: one neuron for a missing or existing state, and one neuron for the value of the continuous column itself.</span></span> <span data-ttu-id="699d7-127">予測可能な列のセットを確認することによって 500 を超える出力ニューロンが生成された場合は、 [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] によって追加の出力ニューロンを表す新しいネットワークがマイニング モデル内に生成されます。</span><span class="sxs-lookup"><span data-stu-id="699d7-127">If more than 500 output neurons are generated by reviewing the set of predictable columns, [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] generates a new network in the mining model to represent the additional output neurons.</span></span>  
  
 <span data-ttu-id="699d7-128">ニューロンは、どのネットワーク層に含まれているかに応じて、他のニューロンまたは他のデータから入力を受け取ります。</span><span class="sxs-lookup"><span data-stu-id="699d7-128">A neuron receives input from other neurons, or from other data, depending on which layer of the network it is in.</span></span> <span data-ttu-id="699d7-129">入力ニューロンでは、元のデータからの入力を受け取ります。</span><span class="sxs-lookup"><span data-stu-id="699d7-129">An input neuron receives inputs from the original data.</span></span> <span data-ttu-id="699d7-130">非表示ニューロンと出力ニューロンは、ニューラル ネットワーク内の他のニューロンの出力からの入力を受け取ります。</span><span class="sxs-lookup"><span data-stu-id="699d7-130">Hidden neurons and output neurons receive inputs from the output of other neurons in the neural network.</span></span> <span data-ttu-id="699d7-131">入力によってニューロン間のリレーションシップが確立され、特定のケース セットを分析するためのパスとしての役割を果たします。</span><span class="sxs-lookup"><span data-stu-id="699d7-131">Inputs establish relationships between neurons, and the relationships serve as a path of analysis for a specific set of cases.</span></span>  
  
 <span data-ttu-id="699d7-132">各入力には、 *重み*と呼ばれる値が割り当てられています。この値は、非表示ニューロンまたは出力ニューロンに対する特定の入力の関連性または重要性を表します。</span><span class="sxs-lookup"><span data-stu-id="699d7-132">Each input has a value assigned to it, called the *weight*, which describes the relevance or importance of that particular input to the hidden neuron or the output neuron.</span></span> <span data-ttu-id="699d7-133">入力に割り当てられている重みが大きいほど、その入力の値の関連性または重要性が増加します。</span><span class="sxs-lookup"><span data-stu-id="699d7-133">The greater the weight that is assigned to an input, the more relevant or important the value of that input.</span></span> <span data-ttu-id="699d7-134">重みには負の値も使用できます。これは、その入力によって特定のニューロンがアクティブになるのではなく、抑制されることを意味します。</span><span class="sxs-lookup"><span data-stu-id="699d7-134">Weights can be negative, which implies that the input can inhibit, rather than activate, a specific neuron.</span></span> <span data-ttu-id="699d7-135">特定のニューロンに対する入力の重要性を増加させるには、各入力の値と重みを乗算します。</span><span class="sxs-lookup"><span data-stu-id="699d7-135">The value of each input is multiplied by the weight to emphasize the importance of an input for a specific neuron.</span></span> <span data-ttu-id="699d7-136">負の重みの場合は、値と重みを乗算すると重要性が低下します。</span><span class="sxs-lookup"><span data-stu-id="699d7-136">For negative weights, the effect of multiplying the value by the weight is to deemphasize the importance.</span></span>  
  
 <span data-ttu-id="699d7-137">各ニューロンには *アクティブ化関数*と呼ばれる単純な非線形関数が割り当てられており、ニューラル ネットワークの層に対する特定のニューロンの関連性または重要性が表されます。</span><span class="sxs-lookup"><span data-stu-id="699d7-137">Each neuron has a simple non-linear function assigned to it, called the *activation function*, which describes the relevance or importance of a particular neuron to that layer of a neural network.</span></span> <span data-ttu-id="699d7-138">非表示ニューロンではアクティブ化関数として *ハイパーボリック タンジェント* 関数 () を使用し、出力ニューロンではアクティブ化関数として *シグモイド* 関数を使用します。</span><span class="sxs-lookup"><span data-stu-id="699d7-138">Hidden neurons use a *hyperbolic tangent* function (tanh) for their activation function, whereas output neurons use a *sigmoid* function for activation.</span></span> <span data-ttu-id="699d7-139">どちらの関数も、ニューラル ネットワークによる入力ニューロンと出力ニューロン間の非線形リレーションシップのモデル化を可能にする非線形の連続関数です。</span><span class="sxs-lookup"><span data-stu-id="699d7-139">Both functions are nonlinear, continuous functions that allow the neural network to model nonlinear relationships between input and output neurons.</span></span>  
  
### <a name="training-neural-networks"></a><span data-ttu-id="699d7-140">ニューラル ネットワークのトレーニング</span><span class="sxs-lookup"><span data-stu-id="699d7-140">Training Neural Networks</span></span>  
 <span data-ttu-id="699d7-141">[!INCLUDE[msCoName](../../includes/msconame-md.md)] ニューラル ネットワーク アルゴリズムを使用するデータ マイニング モデルのトレーニングには、複数の手順が必要です。</span><span class="sxs-lookup"><span data-stu-id="699d7-141">Several steps are involved in training a data mining model that uses the [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network algorithm.</span></span> <span data-ttu-id="699d7-142">これらの手順は、アルゴリズム パラメーターに対して指定した値の影響を強く受けます。</span><span class="sxs-lookup"><span data-stu-id="699d7-142">These steps are heavily influenced by the values that you specify for the algorithm parameters.</span></span>  
  
 <span data-ttu-id="699d7-143">まず、アルゴリズムによってデータ ソースのトレーニング データの評価および抽出が行われます。</span><span class="sxs-lookup"><span data-stu-id="699d7-143">The algorithm first evaluates and extracts training data from the data source.</span></span> <span data-ttu-id="699d7-144">*予約データ*と呼ばれるトレーニング データの割合は、ネットワークの精度の評価で使用するために予約されています。</span><span class="sxs-lookup"><span data-stu-id="699d7-144">A percentage of the training data, called the *holdout data*, is reserved for use in assessing the accuracy of the network.</span></span> <span data-ttu-id="699d7-145">トレーニング処理の間、トレーニング データを反復処理するたびに、ネットワークが即座に評価されます。</span><span class="sxs-lookup"><span data-stu-id="699d7-145">Throughout the training process, the network is evaluated immediately after each iteration through the training data.</span></span> <span data-ttu-id="699d7-146">精度が向上しなくなると、トレーニング処理が停止します。</span><span class="sxs-lookup"><span data-stu-id="699d7-146">When the accuracy no longer increases, the training process is stopped.</span></span>  
  
 <span data-ttu-id="699d7-147">*SAMPLE_SIZE* パラメーターと *HOLDOUT_PERCENTAGE* パラメーターの値は、トレーニング データからサンプリングするケースの数と、予約データに対して使用しないケースの数を決めるために使用します。</span><span class="sxs-lookup"><span data-stu-id="699d7-147">The values of the *SAMPLE_SIZE* and *HOLDOUT_PERCENTAGE* parameters are used to determine the number of cases to sample from the training data and the number of cases to be put aside for the holdout data.</span></span> <span data-ttu-id="699d7-148">*HOLDOUT_SEED* パラメーターの値は、予約データに対して使用しない個々のケースをランダムに決定するために使用します。</span><span class="sxs-lookup"><span data-stu-id="699d7-148">The value of the *HOLDOUT_SEED* parameter is used to randomly determine the individual cases to be put aside for the holdout data.</span></span>  
  
> [!NOTE]  
>  <span data-ttu-id="699d7-149">これらのアルゴリズム パラメーターは、テスト データ セットを定義するためにマイニング構造に適用される HOLDOUT_SIZE プロパティおよび HOLDOUT_SEED プロパティとは異なります。</span><span class="sxs-lookup"><span data-stu-id="699d7-149">These algorithm parameters are different from the HOLDOUT_SIZE and HOLDOUT_SEED properties, which are applied to a mining structure to define a testing data set.</span></span>  
  
 <span data-ttu-id="699d7-150">次に、マイニング モデルがサポートするネットワークの数と複雑さがアルゴリズムによって決定されます。</span><span class="sxs-lookup"><span data-stu-id="699d7-150">The algorithm next determines the number and complexity of the networks that the mining model supports.</span></span> <span data-ttu-id="699d7-151">予測のみに使用される 1 つまたは複数の属性がマイニング モデルに含まれている場合は、そのようなすべての属性を表す単一のネットワークが作成されます。</span><span class="sxs-lookup"><span data-stu-id="699d7-151">If the mining model contains one or more attributes that are used only for prediction, the algorithm creates a single network that represents all such attributes.</span></span> <span data-ttu-id="699d7-152">入力と予測に使用される 1 つまたは複数の属性がマイニング モデルに含まれている場合は、アルゴリズム プロバイダーによって各属性のネットワークが作成されます。</span><span class="sxs-lookup"><span data-stu-id="699d7-152">If the mining model contains one or more attributes that are used for both input and prediction, the algorithm provider constructs a network for each attribute.</span></span>  
  
 <span data-ttu-id="699d7-153">不連続の値を持つ入力属性および予測可能属性の場合、各入力ニューロンまたは出力ニューロンはそれぞれ 1 つの状態を表します。</span><span class="sxs-lookup"><span data-stu-id="699d7-153">For input and predictable attributes that have discrete values, each input or output neuron respectively represents a single state.</span></span> <span data-ttu-id="699d7-154">連続する値を持つ入力属性および予測可能属性の場合、各入力ニューロンまたは出力ニューロンはそれぞれ属性の値の範囲および分布を表します。</span><span class="sxs-lookup"><span data-stu-id="699d7-154">For input and predictable attributes that have continuous values, each input or output neuron respectively represents the range and distribution of values for the attribute.</span></span> <span data-ttu-id="699d7-155">両方のケースでサポートされる状態の最大数は、 *MAXIMUM_STATES* アルゴリズム パラメーターの値によって異なります。</span><span class="sxs-lookup"><span data-stu-id="699d7-155">The maximum number of states that is supported in either case depends on the value of the *MAXIMUM_STATES* algorithm parameter.</span></span> <span data-ttu-id="699d7-156">特定の属性の状態の数が *MAXIMUM_STATES* アルゴリズム パラメーターの値を超えている場合は、許可された状態の最大数に達するまで、その属性の最も一般的な状態または最も関連性の強い状態が選択され、残りの状態は分析の目的では不足値としてグループ化されます。</span><span class="sxs-lookup"><span data-stu-id="699d7-156">If the number of states for a specific attribute exceeds the value of the *MAXIMUM_STATES* algorithm parameter, the most popular or relevant states for that attribute are chosen, up to the maximum number of states allowed, and the remaining states are grouped as missing values for the purposes of analysis.</span></span>  
  
 <span data-ttu-id="699d7-157">アルゴリズムでは、非表示層に対して作成するニューロンの最初の数を決定するときに *HIDDEN_NODE_RATIO* パラメーターの値が使用されます。</span><span class="sxs-lookup"><span data-stu-id="699d7-157">The algorithm then uses the value of the *HIDDEN_NODE_RATIO* parameter when determining the initial number of neurons to create for the hidden layer.</span></span> <span data-ttu-id="699d7-158">*HIDDEN_NODE_RATIO* を 0 に設定すると、マイニング モデルに対して生成されるネットワーク内に非表示層が作成されず、ニューラル ネットワークがロジスティック回帰として扱われます。</span><span class="sxs-lookup"><span data-stu-id="699d7-158">You can set *HIDDEN_NODE_RATIO* to 0 to prevent the creation of a hidden layer in the networks that the algorithm generates for the mining model, to treat the neural network as a logistic regression.</span></span>  
  
 <span data-ttu-id="699d7-159">アルゴリズム プロバイダーでは、 *一括学習*と呼ばれる処理で、以前に予約されたトレーニング データのセットを取得し、予約データ内の各ケースの実際の既知の値をネットワークの予測と比較することによって、ネットワーク内のすべての入力の重みを同時に反復的に評価します。</span><span class="sxs-lookup"><span data-stu-id="699d7-159">The algorithm provider iteratively evaluates the weight for all inputs across the network at the same time, by taking the set of training data that was reserved earlier and comparing the actual known value for each case in the holdout data with the network's prediction, in a process known as *batch learning*.</span></span> <span data-ttu-id="699d7-160">アルゴリズムによってトレーニング データのセット全体が評価された後、各ニューロンの予測値と実際値が確認されます。</span><span class="sxs-lookup"><span data-stu-id="699d7-160">After the algorithm has evaluated the entire set of training data, the algorithm reviews the predicted and actual value for each neuron.</span></span> <span data-ttu-id="699d7-161">アルゴリズムでは、エラーがあればエラーの程度を計算し、 *バックプロパゲーション*と呼ばれる処理で出力ニューロンから入力ニューロンに逆方向の処理を行い、そのニューロンの入力に関連付けられた重みを調整します。</span><span class="sxs-lookup"><span data-stu-id="699d7-161">The algorithm calculates the degree of error, if any, and adjusts the weights that are associated with the inputs for that neuron, working backward from output neurons to input neurons in a process known as *backpropagation*.</span></span> <span data-ttu-id="699d7-162">次に、アルゴリズムではトレーニング データのセット全体で処理が繰り返されます。</span><span class="sxs-lookup"><span data-stu-id="699d7-162">The algorithm then repeats the process over the entire set of training data.</span></span> <span data-ttu-id="699d7-163">アルゴリズムでは多数の重みと出力ニューロンをサポートできるので、入力の重みの割り当ておよび評価を行うトレーニング処理の実行には、共役勾配アルゴリズムが使用されます。</span><span class="sxs-lookup"><span data-stu-id="699d7-163">Because the algorithm can support many weights and output neurons, the conjugate gradient algorithm is used to guide the training process for assigning and evaluating weights for inputs.</span></span> <span data-ttu-id="699d7-164">共役勾配アルゴリズムの詳細については、このマニュアルでは扱いません。</span><span class="sxs-lookup"><span data-stu-id="699d7-164">A discussion of the conjugate gradient algorithm is outside the scope of this documentation.</span></span>  
  
### <a name="feature-selection"></a><span data-ttu-id="699d7-165">特徴選択</span><span class="sxs-lookup"><span data-stu-id="699d7-165">Feature Selection</span></span>  
 <span data-ttu-id="699d7-166">入力属性の数が *MAXIMUM_INPUT_ATTRIBUTES* パラメーターの値を上回っているか、予測可能属性の数が *MAXIMUM_OUTPUT_ATTRIBUTES* パラメーターの値を上回っている場合は、マイニング モデルに含まれているネットワークの複雑さを軽減するために、機能選択アルゴリズムが使用されます。</span><span class="sxs-lookup"><span data-stu-id="699d7-166">If the number of input attributes is greater than the value of the *MAXIMUM_INPUT_ATTRIBUTES* parameter, or if the number of predictable attributes is greater than the value of the *MAXIMUM_OUTPUT_ATTRIBUTES* parameter, a feature selection algorithm is used to reduce the complexity of the networks that are included in the mining model.</span></span> <span data-ttu-id="699d7-167">機能選択によって、入力属性または予測可能属性の数が、モデルとの関連性が統計的に最も高い属性の数まで減らされます。</span><span class="sxs-lookup"><span data-stu-id="699d7-167">Feature selection reduces the number of input or predictable attributes to those that are most statistically relevant to the model.</span></span>  
  
 <span data-ttu-id="699d7-168">すべての [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] データ マイニング アルゴリズムでは、分析の向上と処理負荷の削減のため、機能の選択が自動的に使用されます。</span><span class="sxs-lookup"><span data-stu-id="699d7-168">Feature selection is used automatically by all [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] data mining algorithms to improve analysis and reduce processing load.</span></span> <span data-ttu-id="699d7-169">ニューラル ネットワーク モデルの機能の選択に使用される方法は、属性のデータ型に応じて異なります。</span><span class="sxs-lookup"><span data-stu-id="699d7-169">The method used for feature selection in neural network models depends on the data type of the attribute.</span></span> <span data-ttu-id="699d7-170">参考に、ニューラル ネットワーク モデルで使用される機能の選択の方法と、ニューラル ネットワーク アルゴリズムに基づくロジスティック回帰アルゴリズムで使用される機能の選択の方法を次の表に示します。</span><span class="sxs-lookup"><span data-stu-id="699d7-170">For reference, the following table shows the feature selection methods used for neural network models, and also shows the feature selection methods used for the Logistic Regression algorithm, which is based on the Neural Network algorithm.</span></span>  
  
|<span data-ttu-id="699d7-171">アルゴリズム</span><span class="sxs-lookup"><span data-stu-id="699d7-171">Algorithm</span></span>|<span data-ttu-id="699d7-172">分析の方法</span><span class="sxs-lookup"><span data-stu-id="699d7-172">Method of analysis</span></span>|<span data-ttu-id="699d7-173">説明</span><span class="sxs-lookup"><span data-stu-id="699d7-173">Comments</span></span>|  
|---------------|------------------------|--------------|  
|<span data-ttu-id="699d7-174">ニューラル ネットワーク</span><span class="sxs-lookup"><span data-stu-id="699d7-174">Neural Network</span></span>|<span data-ttu-id="699d7-175">興味深さのスコア</span><span class="sxs-lookup"><span data-stu-id="699d7-175">Interestingness score</span></span><br /><br /> <span data-ttu-id="699d7-176">Shannon のエントロピー</span><span class="sxs-lookup"><span data-stu-id="699d7-176">Shannon's Entropy</span></span><br /><br /> <span data-ttu-id="699d7-177">K2 事前分布を指定したベイズ定理</span><span class="sxs-lookup"><span data-stu-id="699d7-177">Bayesian with K2 Prior</span></span><br /><br /> <span data-ttu-id="699d7-178">均一な事前分布を指定したベイズ ディリクレ等式 (既定値)</span><span class="sxs-lookup"><span data-stu-id="699d7-178">Bayesian Dirichlet with uniform prior (default)</span></span>|<span data-ttu-id="699d7-179">ニューラル ネットワーク アルゴリズムでは、データに連続列が含まれている限り、エントロピに基づくスコアリング方法とベイズ スコアリング方法の両方を使用できます。</span><span class="sxs-lookup"><span data-stu-id="699d7-179">The Neural Networks algorithm can use both entropy-based and Bayesian scoring methods, as long as the data contains continuous columns.</span></span><br /><br /> <span data-ttu-id="699d7-180">既定モード。</span><span class="sxs-lookup"><span data-stu-id="699d7-180">Default.</span></span>|  
|<span data-ttu-id="699d7-181">ロジスティック回帰</span><span class="sxs-lookup"><span data-stu-id="699d7-181">Logistic Regression</span></span>|<span data-ttu-id="699d7-182">興味深さのスコア</span><span class="sxs-lookup"><span data-stu-id="699d7-182">Interestingness score</span></span><br /><br /> <span data-ttu-id="699d7-183">Shannon のエントロピー</span><span class="sxs-lookup"><span data-stu-id="699d7-183">Shannon's Entropy</span></span><br /><br /> <span data-ttu-id="699d7-184">K2 事前分布を指定したベイズ定理</span><span class="sxs-lookup"><span data-stu-id="699d7-184">Bayesian with K2 Prior</span></span><br /><br /> <span data-ttu-id="699d7-185">均一な事前分布を指定したベイズ ディリクレ等式 (既定値)</span><span class="sxs-lookup"><span data-stu-id="699d7-185">Bayesian Dirichlet with uniform prior (default)</span></span>|<span data-ttu-id="699d7-186">このアルゴリズムでは、機能の選択の動作を制御するパラメーターを渡すことができないため、既定値が使用されます。</span><span class="sxs-lookup"><span data-stu-id="699d7-186">Because you cannot pass a parameter to this algorithm to control feature election behavior, the defaults are used.</span></span> <span data-ttu-id="699d7-187">したがって、すべての属性が不連続属性または分離された属性の場合は、既定値は BDEU です。</span><span class="sxs-lookup"><span data-stu-id="699d7-187">Therefore, if all attributes are discrete or discretized, the default is BDEU.</span></span>|  
  
 <span data-ttu-id="699d7-188">ニューラル ネットワーク モデルに対する機能の選択を制御するアルゴリズム パラメーターは、MAXIMUM_INPUT_ATTRIBUTES、MAXIMUM_OUTPUT_ATTRIBUTES、および MAXIMUM_STATES です。</span><span class="sxs-lookup"><span data-stu-id="699d7-188">The algorithm parameters that control feature selection for a neural network model are MAXIMUM_INPUT_ATTRIBUTES, MAXIMUM_OUTPUT_ATTRIBUTES, and MAXIMUM_STATES.</span></span> <span data-ttu-id="699d7-189">HIDDEN_NODE_RATIO パラメーターを設定して、非表示層の数を制御することもできます。</span><span class="sxs-lookup"><span data-stu-id="699d7-189">You can also control the number of hidden layers by setting the HIDDEN_NODE_RATIO parameter.</span></span>  
  
### <a name="scoring-methods"></a><span data-ttu-id="699d7-190">スコアリング方法</span><span class="sxs-lookup"><span data-stu-id="699d7-190">Scoring Methods</span></span>  
 <span data-ttu-id="699d7-191">*スコアリング* は正規化の一種であり、ニュートラル ネットワーク モデルのトレーニングのコンテキストでは、不連続テキスト ラベルなどの値を、他の種類の入力との比較およびネットワーク内での重み付けを行うことができる値に変換するプロセスを意味します。</span><span class="sxs-lookup"><span data-stu-id="699d7-191">*Scoring* is a kind of normalization, which in the context of training a neural network model means the process of converting a value, such as a discrete text label, into a value that can be compared with other types of inputs and weighted in the network.</span></span> <span data-ttu-id="699d7-192">たとえば、Male および Female という値をとる Gender という入力属性と、可変値をとる Income という入力属性があるとします。この場合、各属性の値を直接比較することはできません。そのため、値を共通のスケールにエンコードして、重みを計算できるようにする必要があります。</span><span class="sxs-lookup"><span data-stu-id="699d7-192">For example, if one input attribute is Gender and the possible values are Male and Female, and another input attribute is Income, with a variable range of values, the values for each attribute are not directly comparable, and therefore must be encoded to a common scale so that the weights can be computed.</span></span> <span data-ttu-id="699d7-193">スコアリングは、このような入力を数値 (特に確率範囲) に正規化するプロセスです。</span><span class="sxs-lookup"><span data-stu-id="699d7-193">Scoring is the process of normalizing such inputs to numeric values: specifically, to a probability range.</span></span> <span data-ttu-id="699d7-194">正規化に使用する関数は、極端な値によって分析結果が歪められることがないように、入力値を単一のスケールでより均等に分布させる場合にも役立ちます。</span><span class="sxs-lookup"><span data-stu-id="699d7-194">The functions used for normalization also help to distribute input value more evenly on a uniform scale so that extreme values do not distort the results of analysis.</span></span>  
  
 <span data-ttu-id="699d7-195">ニューラル ネットワークの出力もエンコードされます。</span><span class="sxs-lookup"><span data-stu-id="699d7-195">Outputs of the neural network are also encoded.</span></span> <span data-ttu-id="699d7-196">出力 (予測) に使用するターゲットが 1 つだけ存在する場合、または入力用ではなく予測のみに使用するターゲットが複数存在する場合、モデルは単一のネットワークを作成するため、値を正規化する必要はないと思われるかもしれません。</span><span class="sxs-lookup"><span data-stu-id="699d7-196">When there is a single target for output (that is, prediction), or multiple targets that are used for prediction only and not for input, the model create a single network and it might not seem necessary to normalize the values.</span></span> <span data-ttu-id="699d7-197">しかし、複数の属性が入力および予測に使用される場合、モデルは複数のネットワークを作成する必要があります。したがって、すべての値を正規化し、出力についてもネットワークからの出力時にエンコードする必要があります。</span><span class="sxs-lookup"><span data-stu-id="699d7-197">However, if multiple attributes are used for input and prediction, the model must create multiple networks; therefore, all values must be normalized, and the outputs too must be encoded as they exit the network.</span></span>  
  
 <span data-ttu-id="699d7-198">入力のエンコードは、トレーニング ケース内の各不連続値を合計した値とその重みの乗算を基にしています。</span><span class="sxs-lookup"><span data-stu-id="699d7-198">Encoding for inputs is based on summing each discrete value in the training cases, and multiplying that value by its weight.</span></span> <span data-ttu-id="699d7-199">これは *加重和*と呼ばれます。加重和は非表示層のアクティブ化関数に渡されます。</span><span class="sxs-lookup"><span data-stu-id="699d7-199">This is called a *weighted sum*, which is passed to the activation function in the hidden layer.</span></span> <span data-ttu-id="699d7-200">次に示すように、エンコードには z スコアが使用されます。</span><span class="sxs-lookup"><span data-stu-id="699d7-200">A z-score is used for encoding, as follows:</span></span>  
  
 <span data-ttu-id="699d7-201">**不連続値**</span><span class="sxs-lookup"><span data-stu-id="699d7-201">**Discrete values**</span></span>  
  
 <span data-ttu-id="699d7-202">μ = p-状態の前の確率</span><span class="sxs-lookup"><span data-stu-id="699d7-202">μ = p - the prior probability of a state</span></span>  
  
 <span data-ttu-id="699d7-203">StdDev  = sqrt(p(1-p))</span><span class="sxs-lookup"><span data-stu-id="699d7-203">StdDev  = sqrt(p(1-p))</span></span>  
  
 <span data-ttu-id="699d7-204">**連続値**</span><span class="sxs-lookup"><span data-stu-id="699d7-204">**Continuous values**</span></span>  
  
 <span data-ttu-id="699d7-205">存在する値 = 1-μ/σ</span><span class="sxs-lookup"><span data-stu-id="699d7-205">Value present= 1 - μ/σ</span></span>  
  
 <span data-ttu-id="699d7-206">既存の値はありません =-μ/σ</span><span class="sxs-lookup"><span data-stu-id="699d7-206">No existing value= -μ/σ</span></span>  
  
 <span data-ttu-id="699d7-207">値のエンコードが完了したら、ネットワーク エッジを重みとして使用して入力を加重合計します。</span><span class="sxs-lookup"><span data-stu-id="699d7-207">After the values have been encoded, the inputs go through weighted summing, with network edges as weights.</span></span>  
  
 <span data-ttu-id="699d7-208">出力のエンコードには、シグモイド関数が使用されます。シグモイド関数は、予測に非常に役立つ性質をいくつか備えています。</span><span class="sxs-lookup"><span data-stu-id="699d7-208">Encoding for outputs uses the sigmoid function, which has properties that make it very useful for prediction.</span></span> <span data-ttu-id="699d7-209">このような性質の 1 つとして、元の値のスケーリング方法や値が負であるか正であるかにかかわらず、常に 0 ～ 1 の値を出力するという点が挙げられます。この性質は確率の推定に適しています。</span><span class="sxs-lookup"><span data-stu-id="699d7-209">One such property is that, regardless of how the original values are scaled, and regardless of whether values are negative or positive, the output of this function is always a value between 0 and 1, which is suited for estimating probabilities.</span></span> <span data-ttu-id="699d7-210">もう 1 つの有用な性質は、シグモイド関数にはスムージング効果があるため、値が変曲点から離れるにつれて、値の確率が 0 または 1 の方向へ緩やかに移動するという点です。</span><span class="sxs-lookup"><span data-stu-id="699d7-210">Another useful property is that the sigmoid function has a smoothing effect, so that as values move farther away from point of inflection, the probability for the value moves towards 0 or 1, but slowly.</span></span>  
  
## <a name="customizing-the-neural-network-algorithm"></a><span data-ttu-id="699d7-211">ニューラル ネットワーク アルゴリズムのカスタマイズ</span><span class="sxs-lookup"><span data-stu-id="699d7-211">Customizing the Neural Network Algorithm</span></span>  
 <span data-ttu-id="699d7-212">[!INCLUDE[msCoName](../../includes/msconame-md.md)] ニューラル ネットワーク アルゴリズムでは、結果として得られるマイニング モデルの動作、パフォーマンス、および精度に影響を与えるいくつかのパラメーターがサポートされています。</span><span class="sxs-lookup"><span data-stu-id="699d7-212">The [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network algorithm supports several parameters that affect the behavior, performance, and accuracy of the resulting mining model.</span></span> <span data-ttu-id="699d7-213">列にモデリング フラグを設定するか、または列内の値の処理方法を指定するディストリビューション フラグを設定して、モデルによるデータの処理方法を変更することもできます。</span><span class="sxs-lookup"><span data-stu-id="699d7-213">You can also modify the way that the model processes data by setting modeling flags on columns, or by setting distribution flags to specify how values within the column are handled.</span></span>  
  
### <a name="setting-algorithm-parameters"></a><span data-ttu-id="699d7-214">アルゴリズム パラメーターの設定</span><span class="sxs-lookup"><span data-stu-id="699d7-214">Setting Algorithm Parameters</span></span>  
 <span data-ttu-id="699d7-215">次の表は、Microsoft ニューラル ネットワーク アルゴリズムで使用できるパラメーターを示しています。</span><span class="sxs-lookup"><span data-stu-id="699d7-215">The following table describes the parameters that can be used with the Microsoft Neural Network algorithm.</span></span>  
  
 <span data-ttu-id="699d7-216">HIDDEN_NODE_RATIO</span><span class="sxs-lookup"><span data-stu-id="699d7-216">HIDDEN_NODE_RATIO</span></span>  
 <span data-ttu-id="699d7-217">入力ニューロンおよび出力ニューロンに対する非表示ニューロンの比率を指定します。</span><span class="sxs-lookup"><span data-stu-id="699d7-217">Specifies the ratio of hidden neurons to input and output neurons.</span></span> <span data-ttu-id="699d7-218">次の式で、非表示層のニューロンの最初の数を求めます。</span><span class="sxs-lookup"><span data-stu-id="699d7-218">The following formula determines the initial number of neurons in the hidden layer:</span></span>  
  
 <span data-ttu-id="699d7-219">HIDDEN_NODE_RATIO \* SQRT (入力ニューロンの総数 \* 出力ニューロンの総数)</span><span class="sxs-lookup"><span data-stu-id="699d7-219">HIDDEN_NODE_RATIO \* SQRT(Total input neurons \* Total output neurons)</span></span>  
  
 <span data-ttu-id="699d7-220">既定値は、4.0 です。</span><span class="sxs-lookup"><span data-stu-id="699d7-220">The default value is 4.0.</span></span>  
  
 <span data-ttu-id="699d7-221">HOLDOUT_PERCENTAGE</span><span class="sxs-lookup"><span data-stu-id="699d7-221">HOLDOUT_PERCENTAGE</span></span>  
 <span data-ttu-id="699d7-222">提示されたエラーの計算に使用するトレーニング データ内のケースの割合を指定します。この割合は、マイニング モデルのトレーニング中に停止条件の一部として使用されます。</span><span class="sxs-lookup"><span data-stu-id="699d7-222">Specifies the percentage of cases within the training data used to calculate the holdout error, which is used as part of the stopping criteria while training the mining model.</span></span>  
  
 <span data-ttu-id="699d7-223">既定値は 30 です。</span><span class="sxs-lookup"><span data-stu-id="699d7-223">The default value is 30.</span></span>  
  
 <span data-ttu-id="699d7-224">HOLDOUT_SEED</span><span class="sxs-lookup"><span data-stu-id="699d7-224">HOLDOUT_SEED</span></span>  
 <span data-ttu-id="699d7-225">アルゴリズムが予約データをランダムに調べるときに使用する擬似乱数ジェネレーターのシード値を指定します。</span><span class="sxs-lookup"><span data-stu-id="699d7-225">Specifies a number that is used to seed the pseudo-random generator when the algorithm randomly determines the holdout data.</span></span> <span data-ttu-id="699d7-226">このパラメーターを 0 に設定すると、アルゴリズムによってマイニング モデルの名前に基づいたシードが生成され、再処理中にモデルのコンテンツが変更されることはありません。</span><span class="sxs-lookup"><span data-stu-id="699d7-226">If this parameter is set to 0, the algorithm generates the seed based on the name of the mining model, to guarantee that the model content remains the same during reprocessing.</span></span>  
  
 <span data-ttu-id="699d7-227">既定値は 0 です。</span><span class="sxs-lookup"><span data-stu-id="699d7-227">The default value is 0.</span></span>  
  
 <span data-ttu-id="699d7-228">MAXIMUM_INPUT_ATTRIBUTES</span><span class="sxs-lookup"><span data-stu-id="699d7-228">MAXIMUM_INPUT_ATTRIBUTES</span></span>  
 <span data-ttu-id="699d7-229">機能選択を採用する前にアルゴリズムに指定できる入力属性の最大数を設定します。</span><span class="sxs-lookup"><span data-stu-id="699d7-229">Determines the maximum number of input attributes that can be supplied to the algorithm before feature selection is employed.</span></span> <span data-ttu-id="699d7-230">この値を 0 に設定すると、入力属性に対する機能の選択が無効になります。</span><span class="sxs-lookup"><span data-stu-id="699d7-230">Setting this value to 0 disables feature selection for input attributes.</span></span>  
  
 <span data-ttu-id="699d7-231">既定値は 255 です。</span><span class="sxs-lookup"><span data-stu-id="699d7-231">The default value is 255.</span></span>  
  
 <span data-ttu-id="699d7-232">MAXIMUM_OUTPUT_ATTRIBUTES</span><span class="sxs-lookup"><span data-stu-id="699d7-232">MAXIMUM_OUTPUT_ATTRIBUTES</span></span>  
 <span data-ttu-id="699d7-233">機能選択を採用する前にアルゴリズムに指定できる出力属性の最大数を設定します。</span><span class="sxs-lookup"><span data-stu-id="699d7-233">Determines the maximum number of output attributes that can be supplied to the algorithm before feature selection is employed.</span></span> <span data-ttu-id="699d7-234">この値を 0 に設定すると、出力属性に対する機能の選択が無効になります。</span><span class="sxs-lookup"><span data-stu-id="699d7-234">Setting this value to 0 disables feature selection for output attributes.</span></span>  
  
 <span data-ttu-id="699d7-235">既定値は 255 です。</span><span class="sxs-lookup"><span data-stu-id="699d7-235">The default value is 255.</span></span>  
  
 <span data-ttu-id="699d7-236">MAXIMUM_STATES</span><span class="sxs-lookup"><span data-stu-id="699d7-236">MAXIMUM_STATES</span></span>  
 <span data-ttu-id="699d7-237">アルゴリズムによってサポートされる、属性ごとの不連続状態の最大数を指定します。</span><span class="sxs-lookup"><span data-stu-id="699d7-237">Specifies the maximum number of discrete states per attribute that is supported by the algorithm.</span></span> <span data-ttu-id="699d7-238">特定の属性の状態の数がこのパラメーターで指定した数を上回ると、アルゴリズムはその属性の最も一般的な状態を使用し、残りの状態を存在しない状態として扱います。</span><span class="sxs-lookup"><span data-stu-id="699d7-238">If the number of states for a specific attribute is greater than the number that is specified for this parameter, the algorithm uses the most popular states for that attribute and treats the remaining states as missing.</span></span>  
  
 <span data-ttu-id="699d7-239">既定値は 100 です。</span><span class="sxs-lookup"><span data-stu-id="699d7-239">The default value is 100.</span></span>  
  
 <span data-ttu-id="699d7-240">SAMPLE_SIZE</span><span class="sxs-lookup"><span data-stu-id="699d7-240">SAMPLE_SIZE</span></span>  
 <span data-ttu-id="699d7-241">モデルのトレーニングに使用するケースの数を指定します。</span><span class="sxs-lookup"><span data-stu-id="699d7-241">Specifies the number of cases to be used to train the model.</span></span> <span data-ttu-id="699d7-242">アルゴリズムでは、この数と、HOLDOUT_PERCENTAGE パラメーターで指定された予約データに含まれないケースの総数の割合のうち、いずれか小さい方が使用されます。</span><span class="sxs-lookup"><span data-stu-id="699d7-242">The algorithm uses either this number or the percentage of total of cases not included in the holdout data as specified by the HOLDOUT_PERCENTAGE parameter, whichever value is smaller.</span></span>  
  
 <span data-ttu-id="699d7-243">たとえば、HOLDOUT_PERCENTAGE が 30 に設定されている場合、アルゴリズムでは、このパラメーターの値と、ケースの総数の 70% に相当する値のうち、いずれか小さい方が使用されます。</span><span class="sxs-lookup"><span data-stu-id="699d7-243">In other words, if HOLDOUT_PERCENTAGE is set to 30, the algorithm will use either the value of this parameter, or a value equal to 70 percent of the total number of cases, whichever is smaller.</span></span>  
  
 <span data-ttu-id="699d7-244">既定値は 10000 です。</span><span class="sxs-lookup"><span data-stu-id="699d7-244">The default value is 10000.</span></span>  
  
### <a name="modeling-flags"></a><span data-ttu-id="699d7-245">ModelingFlags</span><span class="sxs-lookup"><span data-stu-id="699d7-245">Modeling Flags</span></span>  
 <span data-ttu-id="699d7-246">[!INCLUDE[msCoName](../../includes/msconame-md.md)] ニューラル ネットワーク アルゴリズムでは、次のモデリング フラグを使用できます。</span><span class="sxs-lookup"><span data-stu-id="699d7-246">The following modeling flags are supported for use with the [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network algorithm.</span></span>  
  
 <span data-ttu-id="699d7-247">NOT NULL</span><span class="sxs-lookup"><span data-stu-id="699d7-247">NOT NULL</span></span>  
 <span data-ttu-id="699d7-248">列に NULL を含めることはできないことを示します。</span><span class="sxs-lookup"><span data-stu-id="699d7-248">Indicates that the column cannot contain a null.</span></span> <span data-ttu-id="699d7-249">モデルのトレーニング中に NULL が検出された場合はエラーが発生します。</span><span class="sxs-lookup"><span data-stu-id="699d7-249">An error will result if Analysis Services encounters a null during model training.</span></span>  
  
 <span data-ttu-id="699d7-250">マイニング構造列に適用されます。</span><span class="sxs-lookup"><span data-stu-id="699d7-250">Applies to mining structure columns.</span></span>  
  
 <span data-ttu-id="699d7-251">MODEL_EXISTENCE_ONLY</span><span class="sxs-lookup"><span data-stu-id="699d7-251">MODEL_EXISTENCE_ONLY</span></span>  
 <span data-ttu-id="699d7-252">属性の値が存在するかどうかだけをモデルで考慮する必要があることを示します。</span><span class="sxs-lookup"><span data-stu-id="699d7-252">Indicates that the model should only consider whether a value exists for the attribute or if a value is missing.</span></span> <span data-ttu-id="699d7-253">正確な値かどうかは問題になりません。</span><span class="sxs-lookup"><span data-stu-id="699d7-253">The exact value does not matter.</span></span>  
  
 <span data-ttu-id="699d7-254">マイニング モデル列に適用されます。</span><span class="sxs-lookup"><span data-stu-id="699d7-254">Applies to mining model columns.</span></span>  
  
### <a name="distribution-flags"></a><span data-ttu-id="699d7-255">ディストリビューション フラグ</span><span class="sxs-lookup"><span data-stu-id="699d7-255">Distribution Flags</span></span>  
 <span data-ttu-id="699d7-256">[!INCLUDE[msCoName](../../includes/msconame-md.md)] ニューラル ネットワーク アルゴリズムでは、次のディストリビューション フラグを使用できます。</span><span class="sxs-lookup"><span data-stu-id="699d7-256">The following distribution flags are supported for use with the [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network algorithm.</span></span> <span data-ttu-id="699d7-257">ディストリビューション フラグは、モデルのみへのヒントとして使用されます。アルゴリズムでは、異なる分布が検出された場合、ヒントで指定された分布ではなく、検出された分布が使用されます。</span><span class="sxs-lookup"><span data-stu-id="699d7-257">The flags are used as hints to the model only; if the algorithm detects a different distribution it will use the found distribution, not the distribution provided in the hint.</span></span>  
  
 <span data-ttu-id="699d7-258">標準</span><span class="sxs-lookup"><span data-stu-id="699d7-258">Normal</span></span>  
 <span data-ttu-id="699d7-259">列内の値を、正規分布またはガウス分布を表す値として処理する必要があることを示します。</span><span class="sxs-lookup"><span data-stu-id="699d7-259">Indicates that values within the column should be treated as though they represent the normal, or Gaussian, distribution.</span></span>  
  
 <span data-ttu-id="699d7-260">Uniform</span><span class="sxs-lookup"><span data-stu-id="699d7-260">Uniform</span></span>  
 <span data-ttu-id="699d7-261">列内の値を、均等に分布している値として処理する必要があることを示します。つまり、値の確率はほぼ均等で、値の合計数の関数になります。</span><span class="sxs-lookup"><span data-stu-id="699d7-261">Indicates that values within the column should be treated as though they are distributed uniformly; that is, the probability of any value is roughly equal, and is a function of the total number of values.</span></span>  
  
 <span data-ttu-id="699d7-262">Log Normal</span><span class="sxs-lookup"><span data-stu-id="699d7-262">Log Normal</span></span>  
 <span data-ttu-id="699d7-263">列内の値を、 *対数正規* 曲線に従って分布している (値の対数が正規分布している) 値として処理する必要があることを示します。</span><span class="sxs-lookup"><span data-stu-id="699d7-263">Indicates that values within the column should be treated as though distributed according to the *log normal* curve, which means that the logarithm of the values is distributed normally.</span></span>  
  
## <a name="requirements"></a><span data-ttu-id="699d7-264">必要条件</span><span class="sxs-lookup"><span data-stu-id="699d7-264">Requirements</span></span>  
 <span data-ttu-id="699d7-265">線形回帰モデルには、1 つ以上の入力列と 1 つの出力列が必要です。</span><span class="sxs-lookup"><span data-stu-id="699d7-265">A neural network model must contain at least one input column and one output column.</span></span>  
  
### <a name="input-and-predictable-columns"></a><span data-ttu-id="699d7-266">入力列と予測可能列</span><span class="sxs-lookup"><span data-stu-id="699d7-266">Input and Predictable Columns</span></span>  
 <span data-ttu-id="699d7-267">[!INCLUDE[msCoName](../../includes/msconame-md.md)] ニューラル ネットワーク アルゴリズムでは、次の表に示す特定の入力列と予測可能列がサポートされています。</span><span class="sxs-lookup"><span data-stu-id="699d7-267">The [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network algorithm supports the specific input columns and predictable columns that are listed in the following table.</span></span>  
  
|<span data-ttu-id="699d7-268">列</span><span class="sxs-lookup"><span data-stu-id="699d7-268">Column</span></span>|<span data-ttu-id="699d7-269">コンテンツの種類</span><span class="sxs-lookup"><span data-stu-id="699d7-269">Content types</span></span>|  
|------------|-------------------|  
|<span data-ttu-id="699d7-270">入力属性</span><span class="sxs-lookup"><span data-stu-id="699d7-270">Input attribute</span></span>|<span data-ttu-id="699d7-271">Continuous、Cyclical、Discrete、Discretized、Key、Table、Ordered</span><span class="sxs-lookup"><span data-stu-id="699d7-271">Continuous, Cyclical, Discrete, Discretized, Key, Table, and Ordered</span></span>|  
|<span data-ttu-id="699d7-272">予測可能な属性</span><span class="sxs-lookup"><span data-stu-id="699d7-272">Predictable attribute</span></span>|<span data-ttu-id="699d7-273">Continuous、Cyclical、Discrete、Discretized、Ordered</span><span class="sxs-lookup"><span data-stu-id="699d7-273">Continuous, Cyclical, Discrete, Discretized, and Ordered</span></span>|  
  
> [!NOTE]  
>  <span data-ttu-id="699d7-274">コンテンツの種類 Cyclical および Ordered はサポートされますが、アルゴリズムはこれらを不連続の値として扱い、特別な処理は行いません。</span><span class="sxs-lookup"><span data-stu-id="699d7-274">Cyclical and Ordered content types are supported, but the algorithm treats them as discrete values and does not perform special processing.</span></span>  
  
## <a name="see-also"></a><span data-ttu-id="699d7-275">参照</span><span class="sxs-lookup"><span data-stu-id="699d7-275">See Also</span></span>  
 <span data-ttu-id="699d7-276">[Microsoft ニューラルネットワークアルゴリズム](microsoft-neural-network-algorithm.md) </span><span class="sxs-lookup"><span data-stu-id="699d7-276">[Microsoft Neural Network Algorithm](microsoft-neural-network-algorithm.md) </span></span>  
 <span data-ttu-id="699d7-277">[ニューラルネットワークモデルのマイニングモデルコンテンツ &#40;Analysis Services データマイニング&#41;](mining-model-content-for-neural-network-models-analysis-services-data-mining.md) </span><span class="sxs-lookup"><span data-stu-id="699d7-277">[Mining Model Content for Neural Network Models &#40;Analysis Services - Data Mining&#41;](mining-model-content-for-neural-network-models-analysis-services-data-mining.md) </span></span>  
 [<span data-ttu-id="699d7-278">Neural Network Model Query Examples</span><span class="sxs-lookup"><span data-stu-id="699d7-278">Neural Network Model Query Examples</span></span>](neural-network-model-query-examples.md)  
  
  
